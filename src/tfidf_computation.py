# ---------------------------------------------
# This file was mostly generated by ChatGPT and altered to fit our purposes.

import pandas as pd
import os
from pyspark.sql import SparkSession, DataFrame
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.sql.functions import col, udf
from pyspark.sql.types import DoubleType


def get_spark() -> SparkSession:
    return (
        SparkSession.builder.appName("myApp")
        .config("spark.driver.memory", "4g")
        .config("spark.sql.caseSensitive", "true")
        .getOrCreate()
    )


def run_tfidf(
    json_path: str,
    id_field: str = "id",
    text_field: str = "title",
    output_tfidf_path: str = "spark/tfidf",
    pipeline_model_path: str = "spark/pipeline_model",
    num_features: int = 1 << 18,
) -> None:
    """
    Reads documents from a JSON file (array of objects), computes TF-IDF on the `text_field`,
    and saves both the TF-IDF vectors and the fitted pipeline model.

    Args:
        spark: Existing SparkSession.
        json_path: Path to a JSON file containing an array of objects.
        id_field: Name of the field to use as identifier.
        text_field: Name of the field containing text to vectorize.
        output_tfidf_path: Filesystem path where TF-IDF DataFrame (id + vector) is saved.
        pipeline_model_path: Filesystem path where the fitted pipeline model is saved.
        num_features: Number of features for HashingTF.
    """
    # Read from JSON, enabling multiline for arrays
    df = get_spark().read.option("multiline", True).json(json_path)

    # Select only id and text columns, drop nulls
    data = df.select(col(id_field).cast("string"), col(text_field)).na.drop()

    # Build a pipeline: tokenize, remove stopwords, hash, idf
    tokenizer = Tokenizer(inputCol=text_field, outputCol="tokens")
    remover = StopWordsRemover(inputCol="tokens", outputCol="filtered")
    hashing_tf = HashingTF(
        inputCol="filtered", outputCol="rawFeatures", numFeatures=num_features
    )
    idf = IDF(inputCol="rawFeatures", outputCol="tfidfFeatures")

    pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf])

    # Fit the pipeline and transform
    model = pipeline.fit(data)
    result = model.transform(data).select(
        col(id_field), col(text_field), col("tfidfFeatures")
    )

    # Save TF-IDF vectors and pipeline model
    result.write.mode("overwrite").parquet(output_tfidf_path)
    model.write().overwrite().save(pipeline_model_path)


def get_tfidf(
    tfidf_df_path: str = "spark/tfidf",
    pipeline_model_path: str = "spark/pipeline_model",
    paper_id: str = None,
    title: str = None,
    id_field: str = "id",
    text_field: str = "title",
):
    """
    Retrieves the TF-IDF vector for a document by its ID (fast lookup) or computes
    it on the fly for a given title (using the saved pipeline). Returns a Spark ML vector.
    """
    if paper_id and not title:
        tfidf_df = get_spark().read.parquet(tfidf_df_path)
        row = (
            tfidf_df.filter(col(id_field) == paper_id).select("tfidfFeatures").collect()
        )
        if not row:
            raise ValueError(f"Document ID {paper_id} not found in TF-IDF store.")
        return row[0]["tfidfFeatures"]
    elif title and not paper_id:
        model = PipelineModel.load(pipeline_model_path)
        single = get_spark().createDataFrame([(title,)], [text_field])
        features = model.transform(single).select("tfidfFeatures").collect()
        return features[0]["tfidfFeatures"]
    else:
        raise ValueError("Provide exactly one of paper_id or title.")

def get_or_train_model(
    model_path: str = "spark/pipeline_model"
) -> PipelineModel:
    """
    Loads a pre-trained pipeline model from the specified path. If it doesn't exist,
    it trains a new model on the provided data and saves it to the same path.
    """
    if os.path.exists(model_path):
        return PipelineModel.load(model_path)
    else:
        # Train a new model using default data
        default_json_path = "data/sample.json" 
        run_tfidf(
            json_path=default_json_path,
            output_tfidf_path="spark/tfidf",
            pipeline_model_path=model_path,
        )
        return PipelineModel.load(model_path)
def compute_score(
    query: str,
    tfidf_df_path: str = "spark/tfidf",
    pipeline_model_path: str = "spark/pipeline_model",
    id_field: str = "id",
    text_field: str = "title",
    top_n: int = 10,
) -> pd.DataFrame:
    """
    Computes cosine similarity scores between `query` and all documents in the TF-IDF store.
    Returns a DataFrame of (id, score) sorted by descending score, limited to `top_n`.

    Args:
        spark: SparkSession used to read models and data.
        query: The search string to score against the corpus.
        tfidf_df_path: Path where precomputed TF-IDF DataFrame is stored.
        pipeline_model_path: Path where the fitted pipeline is saved.
        id_field: Identifier column name.
        text_field: Original text column name (unused here).
        top_n: Number of top results to return.
    """
    # 1) Transform the query into TF-IDF using the saved pipeline
    model = get_or_train_model(pipeline_model_path)
    query_df = get_spark().createDataFrame([(query,)], [text_field])
    q_vec = model.transform(query_df).select("tfidfFeatures").first()["tfidfFeatures"]

    # 2) Define a UDF for cosine similarity against the query vector
    def cosine_sim(v):
        dot = float(v.dot(q_vec))
        norm_d = float(v.norm(2))
        norm_q = float(q_vec.norm(2))
        return dot / (norm_d * norm_q) if norm_d and norm_q else 0.0

    cosine_udf = udf(cosine_sim, DoubleType())

    # 3) Load precomputed TF-IDF and compute scores
    tfidf_df = get_spark().read.parquet(tfidf_df_path)
    scored = tfidf_df.withColumn("score", cosine_udf(col("tfidfFeatures")))

    # 4) Return top N by score
    return (
        scored.select(col(id_field), col(text_field), col("score"))
        .orderBy(col("score").desc())
        .limit(top_n)
        .toPandas()
    )
