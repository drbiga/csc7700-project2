# Project 2

This project aims to create a web-scale search engine for papers using PageRank and TFIDF for information retrieval.

## Dataset

The dataset that will be used was found in [Kaggle](https://www.kaggle.com/datasets/mathurinache/citation-network-dataset?resource=download).
It is composed of almost 4.9 million papers and occupies roughly 14.8 GB of storage. According to the dataset's data card, there are more than 45 million
citation relationships in the dataset.

## Method

The nodes of the graph are going to be the papers, and the edges will represent citations. When the graph is represented using these representations,
the same logic that exists in following links in the web is captured within the academic domain. Therefore, considering google's success using ranking
operations on their data, it is only natural to try and use the same algorithm to search for references.

Also, TF-IDF will be used to search for relationships between the input queries and the paper titles in order to capture meaning.

The final score for the documents given the query will be calculated using a linear combination between paper's page rank and the semantic similarity between query and paper title as shown in the equation below.

$$
    s_d(q, d) = \alpha \cdot tfidf(q) + (1 - \alpha) \cdot pr(d)
$$

Where $q$ is the query, $d$ is a document, $tfidf(q)$ is the TF-IDF score for query $q$ in the entire database, and $pr(d)$ is the page rank for document $d$ in the database.

## Experiments

In order to test various different scenarios, the result sets are going to be compared for various values of weights $\alpha$. Result set difference will be determined, as shown in the equation below, by calculating the absolute value of the difference between the scores of documents in different sets. In case a document exists in one result set and does not exist in another result set, the score for the document in the result set where it does not exist will be zero.

$$
  \begin{equation*}
  \begin{aligned}
    &diff(R_1, R_2) =
  \begin{cases}
     \frac{1}{Z}\sum\limits_{d \in R}{|s(q, d_1) - s(q, d_2)|}, if ~d \in R_1 ~and ~d \in R_2 \\
     \frac{1}{Z}\sum\limits_{d \in R}{|0 - s(q, d_2)|}, if ~d \notin R_1 ~and ~d \in R_2 \\
     \frac{1}{Z}\sum\limits_{d \in R}{|s(q, d_1) - 0|}, if ~d \in R_1 ~and ~d \notin R_2 \\
  \end{cases} \\
    &Z = \sum\limits_{d \in R_1}{s(q, d_1)} + \sum\limits_{d \in R_2}{s(q, d_2)} \\
    &R = R_1 \cup R_2
  \end{aligned}
  \end{equation*}
$$

Where $R_1$ and $R_2$ are result sets, $d$ is a document belonging to $R_1$ or to $R_2$, $s$ is the score of document $d$ in the respective result set, or zero in case document $d$ does not exist in that result set, and $q$ is the input query that generated the aforementioned result sets. A normalizing factor $Z$ was also used to keep the difference constrained between 0 (result sets are identical) and 1 (result sets do not intersect). The above equation could also be expressed by the summation over the intersection set plus all the scores of the documents outside the intersection, but the summation over the union was chosen for clarity and simplicity.

Also, query performance will be evaluated for many different database sizes. Using PageRank, a sample of the database will be taken to contain a subset of the original data. Then, a series of 1000 queries will be run to evaluate the average response time. The queries will be generated by sampling 5 words from the titles of 200 randomly chosen papers among the 10% highest ones scored according to their page ranks.

Lastly, for a number of different values, the average result set size required to contain the input document given a query will be calculated. The most common metrics cannot be used since there is only one "good result" in the system. Therefore, the precision-at-K family of metrics would have only two different values, $\frac{1}{K}$ and $0$. This problem is structural, and a new way to analyze result sets was needed. Then, the simplest metric possible is simply the size of the result set that is required to include the correct document given a query. It could also be seen as the rank (position in the ordered list) of the result document in the result set. This is shown in the equation below.

$$
    s = rank(d, q)
$$

Where $s$ is the newly defined metric and stands for "size", rank is the rank (position) of document $d$ in the entire database ordered by scores, and $q$ is the input query that will be used to compute the scores.
